{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "version": "3.6.4",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "name": "python",
   "mimetype": "text/x-python"
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "\"\"\"NOTES: Batch data is different each time in keras, which result in slight differences in results.\"\"\"\n",
    "import time\n",
    "import pickle\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from keras.layers import Conv1D, Dense, Dropout, MaxPooling1D,Reshape,multiply,Permute,\\\n",
    "              GlobalAveragePooling1D,BatchNormalization,Flatten,UpSampling1D,Conv1DTranspose,\\\n",
    "                Flatten,  Lambda, Input\n",
    "from keras.models import Model,load_model\n",
    "from keras.regularizers import l2\n",
    "from scipy.interpolate import splev, splrep\n",
    "from keras.activations import sigmoid\n",
    "from keras.callbacks import LearningRateScheduler,ModelCheckpoint\n",
    "from keras.utils import np_utils\n",
    "from IPython.display import SVG,display,HTML\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "import keras.backend as K\n",
    "from sklearn.metrics import confusion_matrix,f1_score,roc_auc_score\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import math\n",
    "import tensorflow as tf"
   ],
   "metadata": {
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "execution": {
     "iopub.status.busy": "2022-04-04T08:36:41.24807Z",
     "iopub.execute_input": "2022-04-04T08:36:41.24843Z",
     "iopub.status.idle": "2022-04-04T08:36:41.257779Z",
     "shell.execute_reply.started": "2022-04-04T08:36:41.248395Z",
     "shell.execute_reply": "2022-04-04T08:36:41.257128Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "base_dir = \"../input/sleepapnea\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1\"\n",
    "ir = 3 # interpolate interval\n",
    "before = 2\n",
    "after = 2\n",
    "# normalize\n",
    "scaler = lambda arr: (arr - np.min(arr)) / (np.max(arr) - np.min(arr))\n",
    "def load_data():\n",
    "    path=\"apnea-ecg.pkl\" \n",
    "    tm = np.arange(0, (before + 1 + after) * 60, step=1 / float(ir))\n",
    "    with open(os.path.join(base_dir, path), 'rb') as f: # read preprocessing result\n",
    "        apnea_ecg = pickle.load(f)\n",
    "    x,x_train,x_val = [],[],[]\n",
    "    o_train, y_train = apnea_ecg[\"o_train\"], apnea_ecg[\"y_train\"]\n",
    "    groups_train = apnea_ecg[\"groups_train\"]\n",
    "    for i in range(len(o_train)):\n",
    "        (rri_tm, rri_signal), (ampl_tm, ampl_siganl) = o_train[i]\n",
    "\t\t# Curve interpolation\n",
    "        rri_interp_signal = splev(tm, splrep(rri_tm, scaler(rri_signal), k=3), ext=1)\n",
    "        ampl_interp_signal = splev(tm, splrep(ampl_tm, scaler(ampl_siganl), k=3), ext=1)\n",
    "        x.append([rri_interp_signal, ampl_interp_signal])\n",
    "    groups_training,groups_val=[],[]\n",
    "\n",
    "    num=[i for i in range(16709)]\n",
    "    trainlist, vallist,y_train, y_val = train_test_split(num,y_train, test_size=0.3,random_state=42,stratify =y_train)\n",
    "    print()\n",
    "    for i in trainlist:\n",
    "        x_train.append(x[i])\n",
    "        groups_training.append(groups_train[i])\n",
    "    for i in vallist:\n",
    "        x_val.append(x[i])\n",
    "        groups_val.append(groups_train[i])\n",
    "        \n",
    "    x_train = np.array(x_train, dtype=\"float32\").transpose((0, 2, 1)) # convert to numpy format\n",
    "    y_train= np.array(y_train, dtype=\"float32\")\n",
    "    x_val = np.array(x_val, dtype=\"float32\").transpose((0, 2, 1)) # convert to numpy format\n",
    "    y_val = np.array(y_val, dtype=\"float32\")\n",
    "    \n",
    "    x_test = []\n",
    "    o_test, y_test = apnea_ecg[\"o_test\"], apnea_ecg[\"y_test\"]\n",
    "    groups_test = apnea_ecg[\"groups_test\"]\n",
    "    for i in range(len(o_test)):\n",
    "        (rri_tm, rri_signal), (ampl_tm, ampl_siganl) = o_test[i]\n",
    "\t\t# Curve interpolation\n",
    "        rri_interp_signal = splev(tm, splrep(rri_tm, scaler(rri_signal), k=3), ext=1)\n",
    "        ampl_interp_signal = splev(tm, splrep(ampl_tm, scaler(ampl_siganl), k=3), ext=1)\n",
    "        x_test.append([rri_interp_signal, ampl_interp_signal])\n",
    "    x_test = np.array(x_test, dtype=\"float32\").transpose((0, 2, 1))\n",
    "    y_test = np.array(y_test, dtype=\"float32\")\n",
    "\n",
    "    return x_train,y_train, groups_training,x_val, y_val, groups_val, x_test, y_test, groups_test"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-04-04T08:36:41.290566Z",
     "iopub.execute_input": "2022-04-04T08:36:41.291289Z",
     "iopub.status.idle": "2022-04-04T08:36:41.311901Z",
     "shell.execute_reply.started": "2022-04-04T08:36:41.29125Z",
     "shell.execute_reply": "2022-04-04T08:36:41.310658Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "x_train,y_train, groups_train,x_val, y_val, groups_val, x_test, y_test, groups_test= load_data()\n",
    "y_train = np_utils.to_categorical(y_train, num_classes=2) # Convert to two categories\n",
    "y_val = np_utils.to_categorical(y_val, num_classes=2)\n",
    "y_test = np_utils.to_categorical(y_test, num_classes=2)\n",
    "print('input_shape',x_train.shape)\n",
    "rri_train=np.expand_dims(x_train[:,:,0],axis=2)\n",
    "ampl_train=np.expand_dims(x_train[:,:,1],axis=2)\n",
    "rri_val=np.expand_dims(x_val[:,:,0],axis=2)\n",
    "ampl_val=np.expand_dims(x_val[:,:,1],axis=2)\n",
    "rri_test=np.expand_dims(x_test[:,:,0],axis=2)\n",
    "ampl_test=np.expand_dims(x_test[:,:,1],axis=2)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-04-04T08:47:00.614841Z",
     "iopub.execute_input": "2022-04-04T08:47:00.615149Z",
     "iopub.status.idle": "2022-04-04T08:47:38.877362Z",
     "shell.execute_reply.started": "2022-04-04T08:47:00.615117Z",
     "shell.execute_reply": "2022-04-04T08:47:38.876492Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# BAFNet"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "class ScaledDotProductAttention(keras.layers.Layer):\n",
    "    def __init__(self,\n",
    "                 return_attention=False,\n",
    "                 history_only=False,\n",
    "                 **kwargs):\n",
    "        super(ScaledDotProductAttention, self).__init__(**kwargs)\n",
    "        self.supports_masking = True\n",
    "        self.return_attention = return_attention\n",
    "        self.history_only = history_only\n",
    "        self.intensity = self.attention = None\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            'return_attention': self.return_attention,\n",
    "            'history_only': self.history_only,\n",
    "        }\n",
    "        base_config = super(ScaledDotProductAttention, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        if isinstance(input_shape, list):\n",
    "            query_shape, key_shape, value_shape = input_shape\n",
    "        else:\n",
    "            query_shape = key_shape = value_shape = input_shape\n",
    "        output_shape = query_shape[:-1] + value_shape[-1:]\n",
    "        if self.return_attention:\n",
    "            attention_shape = query_shape[:2] + (key_shape[1],)\n",
    "            return [output_shape, attention_shape]\n",
    "        return output_shape\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        if isinstance(mask, list):\n",
    "            mask = mask[0]\n",
    "        if self.return_attention:\n",
    "            return [mask, None]\n",
    "        return mask\n",
    "\n",
    "    def call(self, inputs, mask=None, **kwargs):\n",
    "        if isinstance(inputs, list):\n",
    "            query, key, value = inputs\n",
    "        else:\n",
    "            query = key = value = inputs\n",
    "        if isinstance(mask, list):\n",
    "            mask = mask[1]\n",
    "        feature_dim = K.shape(query)[-1]\n",
    "        e = K.batch_dot(query, key, axes=2) / K.sqrt(K.cast(feature_dim, dtype=K.floatx()))\n",
    "        if self.history_only:\n",
    "            query_len, key_len = K.shape(query)[1], K.shape(key)[1]\n",
    "            indices = K.expand_dims(K.arange(0, key_len), axis=0)\n",
    "            upper = K.expand_dims(K.arange(0, query_len), axis=-1)\n",
    "            e -= 10000.0 * K.expand_dims(K.cast(indices > upper, K.floatx()), axis=0)\n",
    "        if mask is not None:\n",
    "            e -= 10000.0 * (1.0 - K.cast(K.expand_dims(mask, axis=-2), K.floatx()))\n",
    "        self.intensity = e\n",
    "        e = K.exp(e - K.max(e, axis=-1, keepdims=True))\n",
    "        self.attention = e / K.sum(e, axis=-1, keepdims=True)\n",
    "        v = K.batch_dot(self.attention, value)\n",
    "        if self.return_attention:\n",
    "            return [v, self.attention]\n",
    "        return v"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-04-04T08:37:00.931659Z",
     "iopub.execute_input": "2022-04-04T08:37:00.932209Z",
     "iopub.status.idle": "2022-04-04T08:37:00.951973Z",
     "shell.execute_reply.started": "2022-04-04T08:37:00.932159Z",
     "shell.execute_reply": "2022-04-04T08:37:00.951275Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def create_model(input_shape,weight=1e-3):\n",
    "    inp=Input(shape=input_shape)\n",
    "    input1 =Reshape((900, 1))(inp[:,:,0])\n",
    "    input2 = Reshape((900, 1))(inp[:,:,1])\n",
    "    \n",
    "    x1 = Conv1D(16, kernel_size=11, strides=1, padding=\"same\", activation=\"relu\", kernel_initializer=\"he_normal\",\n",
    "                  kernel_regularizer=l2(weight), bias_regularizer=l2(weight))(input1)\n",
    "    x2 = Conv1D(16, kernel_size=11, strides=1, padding=\"same\", activation=\"relu\", kernel_initializer=\"he_normal\",\n",
    "                  kernel_regularizer=l2(weight), bias_regularizer=l2(weight))(input2)\n",
    "    \n",
    "    x1 = Conv1D(24, kernel_size=11, strides=2, padding=\"same\", activation=\"relu\", kernel_initializer=\"he_normal\",\n",
    "                  kernel_regularizer=l2(1e-3), bias_regularizer=l2(weight))(x1)\n",
    "    x1 = MaxPooling1D(pool_size=3, padding=\"same\")(x1)\n",
    "    x2 = Conv1D(24, kernel_size=11, strides=2, padding=\"same\", activation=\"relu\", kernel_initializer=\"he_normal\",\n",
    "                  kernel_regularizer=l2(1e-3), bias_regularizer=l2(weight))(x2)\n",
    "    x2 = MaxPooling1D(pool_size=3, padding=\"same\")(x2)\n",
    "    fsn2=keras.layers.concatenate([x1, x2], name=\"fsn2\", axis=-1)\n",
    "    \n",
    "    x1 = Conv1D(32 , kernel_size=11, strides=1, padding=\"same\", activation=\"relu\", kernel_initializer=\"he_normal\",\n",
    "                  kernel_regularizer=l2(1e-3), bias_regularizer=l2(weight))(x1)\n",
    "    x1 = MaxPooling1D(pool_size=5, padding=\"same\")(x1)\n",
    "    x2 = Conv1D(32, kernel_size=11, strides=1, padding=\"same\", activation=\"relu\", kernel_initializer=\"he_normal\",\n",
    "                  kernel_regularizer=l2(1e-3), bias_regularizer=l2(weight))(x2)\n",
    "    x2 = MaxPooling1D(pool_size=5, padding=\"same\")(x2)\n",
    "    fsn3=Conv1D(32, kernel_size=11, strides=1, padding=\"same\", activation=\"relu\", kernel_initializer=\"he_normal\",\n",
    "                  kernel_regularizer=l2(1e-3), bias_regularizer=l2(weight))(fsn2)\n",
    "    fsn3=MaxPooling1D(pool_size=5, padding=\"same\")(fsn3)\n",
    "    fsn3=ScaledDotProductAttention()([fsn3,fsn3,fsn3])\n",
    "    x1=ScaledDotProductAttention()([fsn3,x1,x1])\n",
    "    x2=ScaledDotProductAttention()([fsn3,x2,x2])\n",
    "     \n",
    "    # concat\n",
    "    concat = keras.layers.concatenate([x1, x2], name=\"Concat_Layer_x1\", axis=-1)\n",
    "    \n",
    "    # FCN_1\n",
    "    FCN1 = UpSampling1D(5)(x1)\n",
    "    FCN1 = Conv1DTranspose(24, kernel_size=11, strides=1, padding=\"same\", kernel_initializer=\"he_normal\",\n",
    "                              kernel_regularizer=l2(1e-3), bias_regularizer=l2(weight))(FCN1)\n",
    "    FCN1 = UpSampling1D(3)(FCN1)\n",
    "    FCN1 = Conv1DTranspose(16, kernel_size=11, strides=2, padding=\"same\", kernel_initializer=\"he_normal\",\n",
    "                              kernel_regularizer=l2(1e-3), bias_regularizer=l2(weight))(FCN1)\n",
    "    FCN1 = Conv1DTranspose(1, kernel_size=11, strides=1, padding=\"same\", kernel_initializer=\"he_normal\",\n",
    "                              kernel_regularizer=l2(1e-3), bias_regularizer=l2(weight),name='rri')(FCN1)\n",
    "   \n",
    "    # FCN_2\n",
    "    FCN2 = UpSampling1D(5)(x2)\n",
    "    FCN2 = Conv1DTranspose(24, kernel_size=11, strides=1, padding=\"same\", kernel_initializer=\"he_normal\",\n",
    "                              kernel_regularizer=l2(1e-3), bias_regularizer=l2(weight))(FCN2)\n",
    "    FCN2 = UpSampling1D(3)(FCN2)\n",
    "    FCN2 = Conv1DTranspose(16, kernel_size=11, strides=2, padding=\"same\", kernel_initializer=\"he_normal\",\n",
    "                              kernel_regularizer=l2(1e-3), bias_regularizer=l2(weight))(FCN2)\n",
    "    FCN2 = Conv1DTranspose(1, kernel_size=11, strides=1, padding=\"same\", kernel_initializer=\"he_normal\",\n",
    "                              kernel_regularizer=l2(1e-3), bias_regularizer=l2(weight),name='ampl')(FCN2)\n",
    "    \n",
    "    #Channel-wise fusion module\n",
    "    squeeze = GlobalAveragePooling1D()(concat)\n",
    "    excitation=Dense(32,activation='relu')(squeeze)\n",
    "    excitation=Dense(64,activation='sigmoid')(excitation)\n",
    "    excitation = Reshape((1, 64))(excitation)\n",
    "    scale = multiply([concat, excitation])\n",
    "    \n",
    "    # Classification\n",
    "    x = GlobalAveragePooling1D(name='GAP')(scale)\n",
    "    outputs=Dense(2,activation='softmax',name=\"outputs\")(x)\n",
    "    model = Model(inputs=inp, outputs=[outputs,FCN1,FCN2])\n",
    "    return model"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-04-04T08:37:00.953869Z",
     "iopub.execute_input": "2022-04-04T08:37:00.954452Z",
     "iopub.status.idle": "2022-04-04T08:37:00.984601Z",
     "shell.execute_reply.started": "2022-04-04T08:37:00.954405Z",
     "shell.execute_reply": "2022-04-04T08:37:00.983721Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Training stage 1"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "def lr_schedule(epoch, lr):\n",
    "    if epoch > 70 and (epoch - 1) % 10 == 0:\n",
    "        lr *= 0.1\n",
    "    print(\"Learning rate: \", lr)\n",
    "    return lr\n",
    "\n",
    "def plot(history):\n",
    "    \"\"\"Plot performance curve\"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "    print(history)\n",
    "    axes[0].plot(history[\"outputs_loss\"], \"r-\", history[\"val_outputs_loss\"], \"b-\", linewidth=0.5)\n",
    "    axes[0].set_title(\"Loss\")\n",
    "    axes[1].plot(history[\"outputs_accuracy\"], \"r-\", history[\"val_outputs_accuracy\"], \"b-\", linewidth=0.5)\n",
    "    axes[1].set_title(\"Accuracy\")\n",
    "    fig.tight_layout()\n",
    "    fig.show()"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-04-04T08:37:00.989077Z",
     "iopub.execute_input": "2022-04-04T08:37:00.989723Z",
     "iopub.status.idle": "2022-04-04T08:37:01.002536Z",
     "shell.execute_reply.started": "2022-04-04T08:37:00.989588Z",
     "shell.execute_reply": "2022-04-04T08:37:01.001767Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "if __name__ == \"__main__\":\n",
    "    model=create_model(x_train.shape[1:])\n",
    "    # compile\n",
    "    model.compile(loss={'outputs': 'binary_crossentropy','rri':'mean_squared_error','ampl':'mean_squared_error'},loss_weights={\n",
    "                  'outputs': 1,'rri': 1,'ampl':1}, optimizer='adam', metrics={'outputs':'accuracy'})\n",
    "    filepath='./BAFNet.hdf5'\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor='val_outputs_accuracy', verbose=1, save_best_only=True,mode='max')\n",
    "    lr_scheduler = LearningRateScheduler(lr_schedule)\n",
    "    callbacks_list = [lr_scheduler, checkpoint]\n",
    "    time_begin = time.time()\n",
    "    history = model.fit(x_train, [y_train,ampl_train,rri_train], batch_size=128, epochs=1,\n",
    "                        validation_data=(x_val, [y_val,ampl_val,rri_val]),callbacks=callbacks_list,steps_per_epoch=92) \n",
    "    time_end = time.time()\n",
    "    t = time_end - time_begin\n",
    "    print('time_train:', t)\n",
    "    plot(history.history)\n",
    "    filepath='./BAFNet.hdf5'\n",
    "    \n",
    "#     test\n",
    "#     model= load_model(filepath,custom_objects={'ScaledDotProductAttention':ScaledDotProductAttention})\n",
    "    \n",
    "    r= model.evaluate(x_test, [y_test,ampl_test,rri_test])\n",
    "    loss=r[0]\n",
    "    acc=r[-1]\n",
    "    # save prediction score\n",
    "    y_score = model.predict(x_test, batch_size=1024, verbose=1)[0]\n",
    "    roc=roc_auc_score(y_score=y_score,y_true=y_test)\n",
    "    output = pd.DataFrame({\"y_true\": y_test[:, 1], \"y_score\": y_score[:, 1], \"subject\": groups_test})\n",
    "    output.to_csv(\"./BAFNet.csv\", index=False)\n",
    "    y=model.predict(x_test, batch_size=1024, verbose=1)[0]\n",
    "    y_true, y_pred = np.argmax(y_test, axis=-1), np.argmax(y, axis=-1)\n",
    "    C = confusion_matrix(y_true, y_pred, labels=(1, 0))\n",
    "    TP, TN, FP, FN = C[0, 0], C[1, 1], C[1, 0], C[0, 1]\n",
    "    acc, sn, sp = 1. * (TP + TN) / (TP + TN + FP + FN), 1. * TP / (TP + FN), 1. * TN / (TN + FP)\n",
    "    f1=f1_score(y_true, y_pred, average='binary')\n",
    "\n",
    "    print(\"TP:{}, TN:{}, FP:{}, FN:{}, loss{}, acc{}, sn{}, sp{}, f1{}, roc{}\".format(TP, TN, FP, FN,loss, acc, sn, sp, f1, roc))"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-04-04T08:54:03.091363Z",
     "iopub.execute_input": "2022-04-04T08:54:03.091881Z",
     "iopub.status.idle": "2022-04-04T08:54:34.186971Z",
     "shell.execute_reply.started": "2022-04-04T08:54:03.09184Z",
     "shell.execute_reply": "2022-04-04T08:54:34.185581Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Hard sample mining : K-means clustering"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "def clustering(X,Y,ratio=1):\n",
    "    from sklearn.cluster import KMeans\n",
    "    import random\n",
    "    Y = np.argmax(Y, axis=1)\n",
    "    rri=X[:,:,0]\n",
    "    clu_rri = KMeans(n_clusters =5).fit(rri)\n",
    "    cluster_labels = clu_rri.labels_\n",
    "    x_train=[]\n",
    "    y_train=[]\n",
    "    dic={}\n",
    "    for i in range(len(rri)):\n",
    "        cl1=cluster_labels[i]\n",
    "        if cl1 in dic.keys():\n",
    "            dic[cl1].append((Y[i],i))\n",
    "        else:\n",
    "            dic[cl1]=[(Y[i],i)]\n",
    "    for key in dic.keys():\n",
    "        G=dic[key]\n",
    "        A=np.sum(G, axis=0)[0]\n",
    "        N=len(G)-A\n",
    "        print(key,\":\",len(G),'N:',N,'SA:',A)\n",
    "        if N>A:\n",
    "            S=1\n",
    "            Num=A\n",
    "        else:\n",
    "            S=0\n",
    "            Num=N\n",
    "        for g in G:\n",
    "            idx=g[1]  \n",
    "            if g[0]==S:\n",
    "                x_train.append(X[idx])\n",
    "                y_train.append(Y[idx])\n",
    "                G.remove(g)\n",
    "        l=random.sample(G, int(ratio*Num))\n",
    "        for g in l:\n",
    "            idx=g[1]\n",
    "            x_train.append(X[idx])\n",
    "            y_train.append(Y[idx])\n",
    "    return np.array(x_train),np.array(y_train)\n",
    "X,Y=clustering(x_train,y_train)\n",
    "Y = np_utils.to_categorical(Y, num_classes=2) # Convert to two categories\n",
    "rri_train=np.expand_dims(X[:,:,0],axis=2)\n",
    "ampl_train=np.expand_dims(X[:,:,1],axis=2)\n",
    "print('ip_shape',X.shape)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-04-04T08:38:28.700223Z",
     "iopub.execute_input": "2022-04-04T08:38:28.700779Z",
     "iopub.status.idle": "2022-04-04T08:38:52.481567Z",
     "shell.execute_reply.started": "2022-04-04T08:38:28.700726Z",
     "shell.execute_reply": "2022-04-04T08:38:52.480495Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Training stage 2"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# filepath='./BAFNet.hdf5'\n",
    "# model = load_model(filepath,custom_objects={'ScaledDotProductAttention':ScaledDotProductAttention})\n",
    "history = model.fit(X, [Y,ampl_train,rri_train], batch_size=128, epochs=5,\n",
    "                        validation_data=(x_val, [y_val,ampl_val,rri_val]),callbacks=callbacks_list) \n",
    "r= model.evaluate(x_test, [y_test,ampl_test,rri_test]) # test the model\n",
    "loss=r[0]\n",
    "acc=r[-1]\n",
    "#save prediction score\n",
    "y_score = model.predict(x_test, batch_size=1024, verbose=1)[0]\n",
    "roc=roc_auc_score(y_score=y_score,y_true=y_test)\n",
    "output = pd.DataFrame({\"y_true\": y_test[:, 1], \"y_score\": y_score[:, 1], \"subject\": groups_test})\n",
    "output.to_csv(\"./BAFNet.csv\", index=False)\n",
    "y=model.predict(x_test, batch_size=1024, verbose=1)[0]\n",
    "y_true, y_pred = np.argmax(y_test, axis=-1), np.argmax(y, axis=-1)\n",
    "C = confusion_matrix(y_true, y_pred, labels=(1, 0))\n",
    "TP, TN, FP, FN = C[0, 0], C[1, 1], C[1, 0], C[0, 1]\n",
    "acc, sn, sp = 1. * (TP + TN) / (TP + TN + FP + FN), 1. * TP / (TP + FN), 1. * TN / (TN + FP)\n",
    "f1=f1_score(y_true, y_pred, average='binary')\n",
    "\n",
    "print(\"TP:{}, TN:{}, FP:{}, FN:{}, loss{}, acc{}, sn{}, sp{}, f1{}, roc{}\".format(TP, TN, FP, FN,loss, acc, sn, sp, f1, roc))"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-04-04T08:45:02.047574Z",
     "iopub.execute_input": "2022-04-04T08:45:02.047866Z",
     "iopub.status.idle": "2022-04-04T08:45:50.692191Z",
     "shell.execute_reply.started": "2022-04-04T08:45:02.047837Z",
     "shell.execute_reply": "2022-04-04T08:45:50.691241Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
